{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW1xoIXhxv2auJflqw5Lin",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barbaroja2000/llm/blob/main/Langchain_%26_Runpod_Meeting_Transcript_Analyser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Langchain Runpod Meeting Transcript Analyser\n",
        "\n",
        "This colab shows how to run Llama2-7b-chat-hf model on runpod. & demo meeting transcript analyser app.\n",
        "\n",
        "*   Participants\n",
        "*   Meeting topic (metadata or parsed)\n",
        "*   Meeting summary\n",
        "*   Meeting date, time, location (metadata or parsed)\n",
        "*   Meeting actions & deadlines\n",
        "*   Decisions Made\n",
        "*   Questions: Raised (and possibly unanswered)\n",
        "\n",
        "Notes:\n",
        "\n",
        "* Embedding model -  \"sentence-transformers/all-mpnet-base-v2\"\n",
        "* FAISS for Vector store - swap out with pinecone for persistant vector store\n",
        "* Model meta-llama/Llama-2-7b-chat-hf\n",
        "* Chunk size for documents 1000 char with 100 overlap\n",
        "* 512 max new tokens Llama-2-7b-chat-hf\n"
      ],
      "metadata": {
        "id": "F0tLNCtO6kBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Keys\n",
        "#@markdown Utitily to load keys from fs, replace with environ vars if not using\n",
        "import os\n",
        "\n",
        "!python -m pip install python-dotenv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "import dotenv\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/keys/keys.env')"
      ],
      "metadata": {
        "id": "8xIXPCQ-6H8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1887f0f2-66c9-4742-9539-a41d3e1bd92e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login ${WANDB_API_KEY}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRFX5c4XFa5X",
        "outputId": "8fd8a02b-cff0-41ed-b2d0-d283be464f48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn on wandb logging for langchain\n",
        "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\""
      ],
      "metadata": {
        "id": "03M86hyNls_i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Synthetic .vtt meeting transcript\n",
        "synthetic_transcript_uri=\"https://gist.githubusercontent.com/barbaroja2000/277fd35e17ae6bc8610c29591f39c3a9/raw/5ecd4dc010e98c54f2d1537835a6acff4317443a/synthetic-transcript\""
      ],
      "metadata": {
        "id": "PplGx_PqAdgK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def fetch_text_file(url, save_path):\n",
        "    \"\"\"\n",
        "    Fetch a text file from a URL and save it locally.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the text file.\n",
        "    - save_path (str): Local path where the file should be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Ensure the request was successful\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Write the content to a local file\n",
        "    with open(save_path, 'w', encoding=response.encoding) as file:\n",
        "        file.write(response.text)\n",
        "\n",
        "\n",
        "save_path = 'synthetic_transcript.vtt'\n",
        "fetch_text_file(synthetic_transcript_uri, save_path)"
      ],
      "metadata": {
        "id": "lzAhnCaHAlAm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU text-generation runpod langchain faiss-cpu huggingface_hub sentence_transformers wandb > /dev/null"
      ],
      "metadata": {
        "id": "wmSnChLV45bO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade 'urllib3<2' #required for AttributeError: module 'urllib3.util' has no attribute 'PROTOCOL_TLS'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-IW7cCge0wq",
        "outputId": "a553033a-b68c-4e98-8c17-5b0fde2150b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (1.26.16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import runpod\n",
        "import os\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "runpod.api_key = os.getenv(\"RUNPOD_API_KEY\", \"your_runpod_api_key\")\n",
        "\n",
        "if runpod.api_key == \"your_runpod_api_key\":\n",
        "    display(\n",
        "        Markdown(\n",
        "            \"It appears that you don't have a RunPod API key. You can obtain one at [runpod.io](https://runpod.io?ref=s7508tca)\"\n",
        "        )\n",
        "    )\n",
        "    raise AssertionError(\"Missing RunPod API key\")"
      ],
      "metadata": {
        "id": "cyv7chPceldR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_count = 1\n",
        "\n",
        "pod = runpod.create_pod(\n",
        "    name=\"Llama-2-7b-chat\",\n",
        "    image_name=\"ghcr.io/huggingface/text-generation-inference:0.9.4\",\n",
        "    gpu_type_id=\"NVIDIA RTX A4500\",\n",
        "    data_center_id=\"EU-RO-1\",\n",
        "    cloud_type=\"SECURE\",\n",
        "    docker_args=f\"--model-id TheBloke/Llama-2-7b-chat-fp16\",\n",
        "    gpu_count=gpu_count,\n",
        "    volume_in_gb=50,\n",
        "    container_disk_in_gb=5,\n",
        "    ports=\"80/http\",\n",
        "    volume_mount_path=\"/data\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srfR5aGCfm11",
        "outputId": "937f6a35-b055-4d28-b6c8-40e44f3cfeb8"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': {'podFindAndDeployOnDemand': {'id': '53tvbee3v9z421', 'imageName': 'ghcr.io/huggingface/text-generation-inference:0.9.4', 'env': [], 'machineId': 'b7yhdwws8mel', 'machine': {'podHostId': '53tvbee3v9z421-64410c16'}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#previously running pod\n",
        "#pod = {}\n",
        "#pod[\"id\"] = \"\""
      ],
      "metadata": {
        "id": "MAckFbu8HvxO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_server_url = f'https://{pod[\"id\"]}-80.proxy.runpod.net'"
      ],
      "metadata": {
        "id": "0qJ3peOp9Mj8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def wait_for_200(url, timeout=60):\n",
        "    while True:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            print(\"Received 200 status code!\")\n",
        "            return\n",
        "        else:\n",
        "            print(f\"Status code: {response.status_code}. Retrying in {timeout} seconds...\")\n",
        "            time.sleep(timeout)\n",
        "\n",
        "wait_for_200(inference_server_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXi2mdBXnPUA",
        "outputId": "84bb5c03-a463-4e3a-aeea-ba4a1e868191"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received 200 status code!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceTextGenInference\n",
        "\n",
        "llm = HuggingFaceTextGenInference(\n",
        "    inference_server_url=inference_server_url,\n",
        "    max_new_tokens=512,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.1,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ],
      "metadata": {
        "id": "41rSlVatjnHk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Docs (Swagger UI) URL: {inference_server_url}/docs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKFMdi0Smvnp",
        "outputId": "cfa42d36-69e0-4013-82f7-4d2e98535ae0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docs (Swagger UI) URL: https://53tvbee3v9z421-80.proxy.runpod.net/docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.llms import HuggingFaceTextGenInference\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import os , requests\n",
        "from typing import List, Dict\n",
        "import glob\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import re"
      ],
      "metadata": {
        "id": "GOm2g30q_1Dq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
      ],
      "metadata": {
        "id": "hGDDmwnhB34A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  SummarizeNQA Class\n",
        "\n",
        "class SummarizeNQA:\n",
        "    def __init__(self, key: str,  llm: HuggingFaceTextGenInference, dir: str) -> None:\n",
        "        if not key:\n",
        "            raise ValueError(\"API key must be provided.\")\n",
        "        if not dir  or not os.path.isdir(dir):\n",
        "            raise ValueError(\"Directory must be provided.\")\n",
        "\n",
        "        self.dir = dir\n",
        "        self.llm = llm\n",
        "\n",
        "        self.db = None\n",
        "        self.docs = None\n",
        "\n",
        "    def load(self, chunk_size: int = 1000, chunk_overlap: int = 100) -> None:\n",
        "\n",
        "        documents = []\n",
        "        if not glob.glob(f\"{self.dir}*.*\"):\n",
        "            raise ValueError(\"Directory must contain at least one file.\")\n",
        "\n",
        "        if  glob.glob(f\"{self.dir}*.vtt\"):\n",
        "          loader = DirectoryLoader(\n",
        "              \"\", glob=f\"{self.dir}*.vtt\", loader_cls=TextLoader\n",
        "          )\n",
        "          documents = [*loader.load(), *documents]\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "        )\n",
        "        self.docs  = text_splitter.split_documents(documents)\n",
        "        embeddings = HuggingFaceEmbeddings( model_name=\"sentence-transformers/all-mpnet-base-v2\") #what other ones to use here\n",
        "        self.db = FAISS.from_documents(self.docs, embeddings)\n",
        "\n",
        "    def summarize(self, max_tokens=1000,chain_type='map_reduce' ):\n",
        "\n",
        "      if not self.db:\n",
        "         raise ValueError(\"Load first\")\n",
        "\n",
        "      map_prompt = \"\"\"\n",
        "                Write a  summary of the following:\n",
        "                \"{text}\"\n",
        "                 SUMMARY:\n",
        "                \"\"\"\n",
        "      map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n",
        "\n",
        "      combine_prompt = \"\"\"\n",
        "      Write a  summary of the following text delimited by triple backquotes.\n",
        "      Return your response in bullet points which covers the key points of the text.\n",
        "      ```{text}```\n",
        "      BULLET POINT  SUMMARY:\n",
        "      \"\"\"\n",
        "      combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n",
        "\n",
        "      summary_chain = load_summarize_chain(llm=self.llm,\n",
        "              chain_type=chain_type,\n",
        "              map_prompt=map_prompt_template,\n",
        "              combine_prompt=combine_prompt_template,\n",
        "              verbose=False, return_intermediate_steps=False\n",
        "          )\n",
        "      response = summary_chain.run(self.docs)\n",
        "      return  self._format(response)\n",
        "\n",
        "    def qa(\n",
        "        self,\n",
        "        query: str,\n",
        "        temperature: float = 0,\n",
        "        count: float = 4,\n",
        "        chain_type: str = \"stuff\", #map_reduce, #refine\n",
        "        return_only_outputs: bool = True,\n",
        "        return_intermediate_steps: bool = False,\n",
        "    ) -> Dict:\n",
        "\n",
        "        docs = self.db.similarity_search(query, k=count)\n",
        "\n",
        "        question_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
        "\n",
        "        {context}\n",
        "        Question: {question}\n",
        "        Relevant text, if any:\"\"\"\n",
        "\n",
        "        QUESTION_PROMPT = PromptTemplate(\n",
        "            template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        combine_prompt_template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer.\n",
        "        If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
        "\n",
        "        QUESTION: {question}\n",
        "        =========\n",
        "        {summaries}\n",
        "        =========\n",
        "        Answer :\"\"\"\n",
        "\n",
        "        COMBINE_PROMPT = PromptTemplate(\n",
        "              template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\n",
        "        )\n",
        "\n",
        "        if chain_type == \"stuff\":\n",
        "            chain = load_qa_chain(self.llm, chain_type=chain_type)\n",
        "        else:\n",
        "            chain =  load_qa_chain(self.llm, chain_type=chain_type, return_intermediate_steps=return_intermediate_steps, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\n",
        "        response =  chain(\n",
        "            {\"input_documents\": docs, \"question\": query},\n",
        "            return_only_outputs=return_only_outputs,\n",
        "        )\n",
        "        return self._format(response[\"output_text\"])\n",
        "\n",
        "    @staticmethod\n",
        "    def _format(answer: str) -> str:\n",
        "        return re.sub('\\n{3,}', '\\n\\n', answer)\n"
      ],
      "metadata": {
        "id": "2Z2rBLYM6iVA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Set Up\n",
        "summarize_and_qa = SummarizeNQA(os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\"),llm=llm, dir=\"./\")"
      ],
      "metadata": {
        "id": "pWJzOLG85qXj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load the texts into documents and index\n",
        "summarize_and_qa.load()"
      ],
      "metadata": {
        "id": "Cqet_uwtQr66"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Summarize\n",
        "# @markdown ```Depending on the length of the text, you may have to reduce the max_token parameter```\n",
        "\n",
        "summary = summarize_and_qa.summarize(max_tokens=1000)"
      ],
      "metadata": {
        "id": "_EMuokDq1vn0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f54f940f-bec0-4fef-af66-0442dd20ae18"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/bandulu/uncategorized/runs/58vieul0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "id": "2EiGXICOB5_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c59a03-46b5-442f-8d49-ed053162f543"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• The meeting discussed the potential of AI in the company's services, specifically in predictive analytics and automation.\n",
            "       • Participants discussed the challenges of differentiation in a crowded market and explored collaboration as a means to gain an edge.\n",
            "       • The importance of technical capabilities to support AI operations, particularly generative models, was highlighted.\n",
            "       • The need for a comprehensive evaluation of the company's infrastructure to support the development of AI products was discussed.\n",
            "       • The possibility of hosting a workshop for clients to demonstrate the company's AI capabilities was raised.\n",
            "       • The importance of continuous learning and collaboration with universities for staying ahead in the AI field was emphasized.\n",
            "       • The integration of generative AI into the company's product lineup and the need for a clear message to communicate the company's capabilities in the AI domain were discussed.\n",
            "       • Ethical implications of generative AI and the importance of ensuring that products are ethically sound were considered.\n",
            "       • The creation of a dedicated AI support team to resolve client issues was proposed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Meeting Participants\n",
        "response = summarize_and_qa.qa(\"list the meeting attendees\",chain_type=\"map_reduce\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "VMt4NJ818aOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84279fc5-5bbd-45cb-d846-04be6194e452"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The answer is:\n",
            "\n",
            "* AC Head of Architecture\n",
            "* BD Managing Director\n",
            "* AJ Principal Architect\n",
            "* MM Enterprise Architect\n",
            "* GC IT Director\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Meeting Topic\n",
        "response = summarize_and_qa.qa(\"what was the topic of the meeting\",chain_type=\"map_reduce\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1wWJezvQEiS",
        "outputId": "d06a631f-3d7f-427f-b350-e14a3743b2be"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ethical implications of generative AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Actions and Deadlines\n",
        "response = summarize_and_qa.qa(\"describe the meeting follow-on actions and any deadlines\",chain_type=\"map_reduce\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "UAJ0eU1G4XRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0727e758-e64c-496c-80d5-792e9d45359d"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The meeting follow-on actions and deadlines are as follows:\n",
            "* Coordinate with the sales team to arrange a workshop on AI capabilities and drive sales in the coming months.\n",
            "* Gather feedback from clients to understand their specific needs in AI solutions.\n",
            "* Draft a proposal for the team structure and responsibilities.\n",
            "* Reconvene in a week to review progress.\n",
            "* A report on infrastructure needs should be ready in about three weeks.\n",
            "* Set up a dedicated team to ensure that AI solutions comply with data protection regulations.\n",
            "* Take action on the idea of continuous learning.\n",
            "* Collaborate with universities to provide training and academic insights.\n",
            "* Consider setting aside a budget for internal research and development.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Decisions Made\n",
        "response = summarize_and_qa.qa(\"List the decisions made in the meeting\",chain_type=\"map_reduce\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "U0nYjjL94mx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e746caa-f729-49ef-e1d5-3d739db6efc6"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The decisions made in the meeting are:\n",
            "\n",
            "A) Setting up a dedicated AI support team\n",
            "B) Drafting a proposal for the team structure and responsibilities\n",
            "C) Needing a mix of technical and domain experts in the team\n",
            "D) Conducting a workshop for clients\n",
            "E) Gathering feedback from clients\n",
            "F) Training is essential\n",
            "G) Evaluating the company's infrastructure needs\n",
            "H) Forming an ethics committee for AI\n",
            "I) Implementing safeguards against misuse of generative AI\n",
            "J) Using \"Ethically Designed AI Solutions\" as a selling point\n",
            "K) Providing post-deployment support for AI solutions\n",
            "\n",
            "Note: The above answer is based on the information provided in the extract and may not reflect the complete picture of the meeting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Questions raised\n",
        "response = summarize_and_qa.qa(\"list all the questions raised in the meeting, and answers provided.\",chain_type=\"map_reduce\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "XS0tnZhf8p4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b813c3d-1c64-4c4c-c9b5-52ce260d9325"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The answers to the questions raised in the meeting are as follows:\n",
            "\n",
            "* Set up a dedicated AI support team: The proposal for a dedicated AI support team was raised, but no answer was provided.\n",
            "* Proposal for team structure and responsibilities: The proposal for a team structure and responsibilities was raised, and it was mentioned that the team will consist of a mix of technical and domain experts.\n",
            "* Review progress in a week: The question of reviewing progress in a week was raised, but no answer was provided.\n",
            "* Infrastructure needs for scalable AI: The need for infrastructure to support scalable AI was raised, and it was discussed that partnerships in the tech space and security concerns will be evaluated.\n",
            "* Evaluation of infrastructure needs: The question of evaluating infrastructure needs was raised, but no answer was provided.\n",
            "* Ethical implications of generative AI: The ethical implications of generative AI were discussed, and it was mentioned that a dedicated ethics committee for AI will be established.\n",
            "* Dedicated ethics committee for AI: The dedication of an ethics committee for AI was raised, but no answer was provided.\n",
            "* Ethically designed AI solutions: The importance of ethically designed AI solutions was discussed, and it was mentioned that post-deployment support for AI solutions will be provided.\n",
            "* Post-deployment support for AI solutions: The question of post-deployment support for AI solutions was raised, but no answer was provided.\n",
            "* Pressing questions raised in the meeting: The pressing questions raised in the meeting were not answered.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the pod\n",
        "runpod.stop_pod(pod[\"id\"])\n",
        "\n",
        "# Terminate the pod\n",
        "runpod.terminate_pod(pod[\"id\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD_s7mgD6PdN",
        "outputId": "ae6c3fba-3440-41a3-df8d-9443cc73d826"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': {'podStop': {'id': '53tvbee3v9z421', 'desiredStatus': 'EXITED'}}}\n",
            "{'data': {'podTerminate': None}}\n"
          ]
        }
      ]
    }
  ]
}