{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQlbuMQkPHgFuanNTZfbOc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barbaroja2000/llm/blob/main/Hugging_Face_%7C_Transformers_Quantized_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers Quantized Models\n",
        "Works for 3b ono size models in Colab with T4|v100,  any bigger than that and session crashes.\n",
        "\n",
        "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing"
      ],
      "metadata": {
        "id": "KCdkp-HPL0NZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbSeL3FpoR4G"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "-RSo6vw_E28L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "qdoBeUeAM1aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Keys\n",
        "#@markdown Utitily to load keys from fs, replace with environ vars if not using\n",
        "\n",
        "import os\n",
        "\n",
        "#os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "!python -m pip install python-dotenv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "import dotenv\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/keys/keys.env')\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= os.environ.get(\"HUGGINGFACE_API_KEY\")"
      ],
      "metadata": {
        "id": "ANpYeScLpYVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sets Transformers cache\n",
        "#@markdown Model reuse\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/models/hf/'\n",
        "\n",
        "!ls -la $TRANSFORMERS_CACHE"
      ],
      "metadata": {
        "id": "50tu1Z5mpJiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10GB\n",
        "#https://huggingface.co/EleutherAI/gpt-neo-2.7B\n",
        "name=\"EleutherAI/gpt-neo-2.7B\""
      ],
      "metadata": {
        "id": "gj6WgpXjs0VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quantization\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model_nf4 = AutoModelForCausalLM.from_pretrained(name, quantization_config=nf4_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)"
      ],
      "metadata": {
        "id": "ZBy7Jn-vpEKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#got gpu\n",
        "model_nf4.hf_device_map"
      ],
      "metadata": {
        "id": "tLR4WepBtWaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Things like gravy go right in among the hairs and stay there. You and\n",
        "I can wipe our smooth faces with a flannel and we quickly look more or less\n",
        "all right again, but the hairy man cannot do that.\n",
        "We can also, if we are careful, eat our meals without spreading food all\n",
        "over our faces. But not so the hairy man. Watch carefully next time you see\n",
        "a hairy man eating his lunch and you will notice that even if he opens his\n",
        "mouth very wide, it is impossible for him to get a spoonful of beef-stew or\n",
        "ice-cream and chocolate sauce into it without leaving some of it on the\n",
        "hairs.\n",
        "Mr Twit didn't even bother to open his mouth wide when he ate. As a result\n",
        "(and because he never washed) there were always hundreds of bits of old\n",
        "breakfasts and lunches and suppers sticking to the hairs around his face.\n",
        "They weren't big bits, mind you, because he used to wipe those off with the\n",
        "back of his hand or on his sleeve while he was eating. But if you looked\n",
        "closely (not that you'd ever want to) you would see tiny little specks of\n",
        "dried-up scrambled eggs stuck to the hairs, and spinach and tomato\n",
        "ketchup and fish fingers and minced chicken livers and all the other\n",
        "disgusting things Mr Twit liked to eat.\n",
        "If you looked closer still (hold your noses, ladies and gentlemen), if you\n",
        "peered deep into the moustachy bristles sticking out over his upper lip, you would\n",
        "probably see much larger objects that had escaped the wipe of his hand,\n",
        "things that had been there for months and months, like a piece of maggoty\n",
        "green cheese or a mouldy old cornflake or even the slimy tail of a tinned\n",
        "sardine.\n",
        "Because of all this, Mr Twit never went really hungry. By sticking out his\n",
        "tongue and curling it sideways to explore the hairy jungle around his mouth,\n",
        "he was always able to find a tasty morsel here and there to nibble on.\n",
        "What I am trying to tell you is that Mr Twit was a foul and smelly old man.\n",
        "He was also an extremely horrid old man, as you will find out in a moment.\"\"\"\n",
        "device = \"cuda:0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "\n",
        "#https://huggingface.co/docs/transformers/main/generation_strategies\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model_nf4.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=1000)\n",
        "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "tMMloyPOsIHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(output_text))                                                                   "
      ],
      "metadata": {
        "id": "2A8tNqu1HJKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ByGflSYwqmxg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}