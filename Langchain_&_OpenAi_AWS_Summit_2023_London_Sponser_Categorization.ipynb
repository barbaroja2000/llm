{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyO7s6sTjy+JiqgTzRFzLKEd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barbaroja2000/llm/blob/main/Langchain_%26_OpenAi_AWS_Summit_2023_London_Sponser_Categorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Langchain & OpenAi - AWS Summit 2023 London Sponser Categorization\n",
        "\n",
        "This notebook categorizes all the companies sponsering the 2023 Summit , into the following categories:\n",
        "\n",
        "* Security\n",
        "* Managed Service Providers (MSPs)\n",
        "* Landing Zone/Infrastructure Providers\n",
        "* Training Partners\n",
        "* Consulting Partners/Systems Integrators\n",
        "* Software/Application Providers\n",
        "* Data Management Providers\n",
        "* Observability\n",
        "* AI/ML\n",
        "\n",
        "Process:\n",
        "\n",
        "1.  Parse the sponser page, pulling out all non AWS pages into a list\n",
        "2. Spider these uris pulling out title & description \n",
        "3. Feed these into a GPT model and categorize\n",
        "4. Display results in a Pandas table\n",
        "\n",
        "Requires OpenAi Key:\n",
        "\n",
        "```Python\n",
        "OPENAI_API_KEY=\"abc\"\n",
        "```"
      ],
      "metadata": {
        "id": "iGxkEOObOZC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Keys\n",
        "#@markdown Utitily to load keys from fs, replace with environ vars if not using\n",
        "\n",
        "import os\n",
        "\n",
        "#os.environ.get(\"OPENAI_API_KEY\")\n",
        "#os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
        "\n",
        "!python -m pip install python-dotenv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "import dotenv\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/keys/keys.env')"
      ],
      "metadata": {
        "id": "T24fRheYQLB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVGA42Bw0U8W"
      },
      "outputs": [],
      "source": [
        "sponser_page = \"https://aws.amazon.com/events/summits/london/sponsors/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse hrefs\n",
        "#@markdown Exclude all local hrefs and anything AWS related\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "candidates = []\n",
        "#required as many pages will return 403 forbidden without user-agent string\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n",
        "result = requests.get(sponser_page, headers=headers)\n",
        "parser = 'html.parser' \n",
        "page = requests.get(sponser_page)\n",
        "soup = BeautifulSoup(page.text, parser)\n",
        "\n",
        "for link in soup.find_all('a', href=True):\n",
        "  x = link['href'].find(\"https://\")\n",
        "  y =  link['href'].find(\"aws\")\n",
        "  if x == 0 and  y ==-1:\n",
        "    domain = link['href'].split(\"/\")\n",
        "    candidates.append(domain[2:3].pop())"
      ],
      "metadata": {
        "id": "Ral81JOW0-cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(candidates[:10])"
      ],
      "metadata": {
        "id": "Vk9qnGdwOE6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Crawl websites\n",
        "#@markdown Pull title and description back from html\n",
        "\n",
        "candidates_for_categorization = []\n",
        "\n",
        "for x,i in enumerate(candidates):\n",
        "  try:\n",
        "    uri = f\"https://{i}\"\n",
        "    resp = requests.get(uri, timeout=10, headers=headers)\n",
        "    soup = BeautifulSoup(resp.text, parser)\n",
        "    description  = soup.find(\"meta\", property=\"og:description\") or soup.find(\"meta\", property=\"description\")\n",
        "    title  = soup.find(\"meta\", property=\"og:title\") or soup.find(\"meta\", property=\"title\") \n",
        "    description = description[\"content\"] if description else None\n",
        "    title = title[\"content\"] if title else None\n",
        "    tmp_dict = {\"title\": title, \"description\": description, \"url\": uri}\n",
        "    candidates_for_categorization.append(tmp_dict)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "e7BkEUbLCLLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(candidates_for_categorization))"
      ],
      "metadata": {
        "id": "bk3mo8tgLph6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai > /dev/null\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "model_name= 'text-davinci-003' #@param [\"text-davinci-003\", \"gpt-4\"]\n",
        "llm = OpenAI(model_name=model_name, temperature=0)"
      ],
      "metadata": {
        "id": "iDF80-7XR9mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Categories listed here are types of cloud companies.\n",
        "\n",
        "Security\n",
        "Managed Service Providers (MSPs)\n",
        "Landing Zone/Infrastructure Providers\n",
        "Training Partners\n",
        "Consulting Partners/Systems Integrators\n",
        "Software/Application Providers\n",
        "Data Management Providers\n",
        "Observability\n",
        "AI/ML\n",
        "\n",
        "Using the description of companies below classify each into the preceeding categories. \n",
        "If the company matches multiple categories, return all matching in a comma seperated list\n",
        "If your confidence is poor for the given classifications, propose a new classification\n",
        "\n",
        "Blurb: {blurb}\n",
        "==============================================================\n",
        "Classification: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"blurb\" ],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "mfUXTbadNFTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = []\n",
        "for i in candidates_for_categorization:\n",
        "  blurb = f\"{i['title']} {i['description']}\"\n",
        "  category = llm_chain.run(blurb)\n",
        "  if \"title\" in i and i[\"title\"] is not None:\n",
        "    i[\"category\"] = [x.strip(' ') for x in category.split(\", \")]\n",
        "    final.append(i)"
      ],
      "metadata": {
        "id": "bNRVq6dySBaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df.drop(df.index, inplace=True)\n",
        "df = pd.DataFrame(final)"
      ],
      "metadata": {
        "id": "4SYkZaZuWUU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ],
      "metadata": {
        "id": "-Kov0-wMYcli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title One hot encode the category column\n",
        "df_ohe = df.drop([\"category\", \"description\", \"company_category\"], 1).join(df.category.str.join('|').str.get_dummies())"
      ],
      "metadata": {
        "id": "euCqgp2sWTwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Filter by Category\n",
        "#@markdown Click on `filter` and enter 1 in the from field against the category\n",
        "df_ohe"
      ],
      "metadata": {
        "id": "gA-kk-6QGalM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sum = df_ohe.drop(['title','url'], 1)"
      ],
      "metadata": {
        "id": "bd3ChOK1FAgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sum Companies by Categorization\n",
        "df_sum.sum()"
      ],
      "metadata": {
        "id": "Hs2FUW0xFgII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save out\n",
        "file_name=f\"aws-summit-sponsers-{model_name}.csv\"\n",
        "df.to_csv(f\"/content/drive/MyDrive/{file_name}\", encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "MnwHT-liYjL-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}