{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSctD8ST80A/gciLrJJitV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barbaroja2000/llm/blob/main/Langchain_Meeting_Transcript_Analyser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Langchain Meeting Transcript Analyser\n",
        "\n",
        "This colab provides a test harness to experiment ewith prompt engineering required to extract the following information from a .vtt file.\n",
        "\n",
        "Participants\n",
        "\n",
        "*   Meeting topic (metadata or parsed)\n",
        "*   Meeting date, time, location (metadata or parsed)\n",
        "*   Meeting actions & deadlines\n",
        "*   Meeting key points\n",
        "*   Decisions Made\n",
        "*   Questions: Raised (and possibly unanswered)"
      ],
      "metadata": {
        "id": "F0tLNCtO6kBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Keys\n",
        "#@markdown Utitily to load keys from fs, replace with environ vars if not using\n",
        "\n",
        "import os\n",
        "\n",
        "#os.environ.get(\"OPENAI_API_KEY\")\n",
        "#os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "!python -m pip install python-dotenv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "import dotenv\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/keys/keys.env')"
      ],
      "metadata": {
        "id": "8xIXPCQ-6H8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1edf9f2-26d0-4d0b-9854-03129ce7ed82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Synthetic .vtt meeting transcript\n",
        "synthetic_transcript_uri=\"https://gist.githubusercontent.com/barbaroja2000/277fd35e17ae6bc8610c29591f39c3a9/raw/5ecd4dc010e98c54f2d1537835a6acff4317443a/synthetic-transcript\""
      ],
      "metadata": {
        "id": "PplGx_PqAdgK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def fetch_text_file(url, save_path):\n",
        "    \"\"\"\n",
        "    Fetch a text file from a URL and save it locally.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the text file.\n",
        "    - save_path (str): Local path where the file should be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Ensure the request was successful\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Write the content to a local file\n",
        "    with open(save_path, 'w', encoding=response.encoding) as file:\n",
        "        file.write(response.text)\n",
        "\n",
        "\n",
        "save_path = 'synthetic_transcript.vtt'\n",
        "fetch_text_file(synthetic_transcript_uri, save_path)"
      ],
      "metadata": {
        "id": "lzAhnCaHAlAm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain faiss-cpu huggingface_hub sentence_transformers > /dev/null"
      ],
      "metadata": {
        "id": "wmSnChLV45bO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "import os , requests\n",
        "from typing import List, Dict\n",
        "import glob\n",
        "from langchain.chains.summarize import load_summarize_chain"
      ],
      "metadata": {
        "id": "GOm2g30q_1Dq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
      ],
      "metadata": {
        "id": "hGDDmwnhB34A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  SummarizeNQA Class\n",
        "\n",
        "class SummarizeNQA:\n",
        "    def __init__(self, key: str, dir: str) -> None:\n",
        "        if not key:\n",
        "            raise ValueError(\"API key must be provided.\")\n",
        "        if not dir  or not os.path.isdir(dir):\n",
        "            raise ValueError(\"Directory must be provided.\")\n",
        "\n",
        "        self.key = key\n",
        "        self.dir = dir\n",
        "        self.db = None\n",
        "        self.docs = None\n",
        "\n",
        "    def load(self, chunk_size: int = 1000, chunk_overlap: int = 100) -> None:\n",
        "\n",
        "        documents = []\n",
        "        if not glob.glob(f\"{self.dir}*.*\"):\n",
        "            raise ValueError(\"Directory must contain at least one file.\")\n",
        "\n",
        "        if  glob.glob(f\"{self.dir}*.vtt\"):\n",
        "          loader = DirectoryLoader(\n",
        "              \"\", glob=f\"{self.dir}*.vtt\", loader_cls=TextLoader\n",
        "          )\n",
        "          documents = [*loader.load(), *documents]\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "        )\n",
        "        self.docs  = text_splitter.split_documents(documents)\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings( model_name=\"sentence-transformers/all-mpnet-base-v2\") #what other ones to use here\n",
        "        self.db = FAISS.from_documents(self.docs, embeddings)\n",
        "\n",
        "    def summarize(self, max_tokens=1000,chain_type='map_reduce' ):\n",
        "\n",
        "      if not self.db:\n",
        "         raise ValueError(\"Load first\")\n",
        "\n",
        "      map_prompt = \"\"\"\n",
        "                Write a  summary of the following:\n",
        "                \"{text}\"\n",
        "                 SUMMARY:\n",
        "                \"\"\"\n",
        "      map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n",
        "\n",
        "      combine_prompt = \"\"\"\n",
        "      Write a  summary of the following text delimited by triple backquotes.\n",
        "      Return your response in bullet points which covers the key points of the text.\n",
        "      ```{text}```\n",
        "      BULLET POINT  SUMMARY:\n",
        "      \"\"\"\n",
        "      combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n",
        "\n",
        "      repo_id = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "\n",
        "      llm = HuggingFaceHub(\n",
        "          huggingfacehub_api_token=self.key,\n",
        "          repo_id=repo_id, model_kwargs={\"max_new_tokens\":256}\n",
        "      )\n",
        "\n",
        "      summary_chain = load_summarize_chain(llm=llm,\n",
        "              chain_type=chain_type,\n",
        "              map_prompt=map_prompt_template,\n",
        "              combine_prompt=combine_prompt_template,\n",
        "              verbose=False, return_intermediate_steps=False\n",
        "          )\n",
        "\n",
        "      return  summary_chain.run(self.docs)\n",
        "\n",
        "    def qa(\n",
        "        self,\n",
        "        query: str,\n",
        "        temperature: float = 0,\n",
        "        count: float = 4,\n",
        "        chain_type: str = \"stuff\", #map_reduce, #refine\n",
        "        return_only_outputs: bool = True,\n",
        "        return_intermediate_steps: bool = False,\n",
        "    ) -> Dict:\n",
        "\n",
        "        docs = self.db.similarity_search(query, k=count)\n",
        "\n",
        "        repo_id = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "\n",
        "        llm = HuggingFaceHub(\n",
        "              huggingfacehub_api_token=self.key,\n",
        "              repo_id=repo_id, model_kwargs={\"max_new_tokens\":256}\n",
        "          )\n",
        "\n",
        "        if chain_type == \"stuff\":\n",
        "            chain = load_qa_with_sources_chain(llm, chain_type=chain_type)\n",
        "        else:\n",
        "            chain =  load_qa_with_sources_chain(llm, chain_type=chain_type, return_intermediate_steps=return_intermediate_steps)\n",
        "        return chain(\n",
        "            {\"input_documents\": docs, \"question\": query},\n",
        "            return_only_outputs=return_only_outputs,\n",
        "        )"
      ],
      "metadata": {
        "id": "2Z2rBLYM6iVA"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Set Up\n",
        "summarize_and_qa = SummarizeNQA(os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\"),\"./\")"
      ],
      "metadata": {
        "id": "pWJzOLG85qXj"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load the texts into documents and index\n",
        "summarize_and_qa.load()"
      ],
      "metadata": {
        "id": "Cqet_uwtQr66"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Summarize\n",
        "# @markdown ```Depending on the length of the text, you may have to reduce the max_token parameter```\n",
        "import re\n",
        "summary = summarize_and_qa.summarize(max_tokens=1500)\n",
        "summary = re.sub('\\n{3,}', '\\n\\n', summary) #summary generates a lot of \\n characters"
      ],
      "metadata": {
        "id": "_EMuokDq1vn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e8a7d88-11a7-4dec-f8b0-f12f493e3eee"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2281 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "id": "2EiGXICOB5_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d998e17a-d12a-4099-b872-956e42ab4a34"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      * IT Director welcomes meeting attendees to discuss new AI strategy for clients.\n",
            "      * Head of Architecture mentions increased client interest in generative AI.\n",
            "      * Principal Architect highlights potential of generative AI in content creation, design, and simulation.\n",
            "      * Managing Director emphasizes importance of universal AI products for clients.\n",
            "      * Enterprise Architect highlights predictive analytics and automation potential in finance and healthcare.\n",
            "      * Head of Marketing stresses importance of differentiation.\n",
            "      * IT Director suggests partnering with industry leaders for competitive edge.\n",
            "      * Principal Architect emphasizes need for robust technical infrastructure to support AI operations.\n",
            "      * AC Head of Architecture suggests dedicated GPU clusters and cloud provider collaborations.\n",
            "      * BD Managing Director tasks AJ Principal Architect with evaluating infrastructure needs and scouting partnerships.\n",
            "      * AJ agrees to collaborate with AC and promises report in three weeks.\n",
            "      * MM Enterprise Architect emphasizes security importance in AI development.\n",
            "      * GC IT Director suggests dedicating team to ensure data protection regulation compliance.\n",
            "      *\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Meeting Participants\n",
        "response = summarize_and_qa.qa(\"list the meeting participants\",chain_type=\"map_reduce\")\n",
        "print(response[\"output_text\"])"
      ],
      "metadata": {
        "id": "VMt4NJ818aOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913f0682-b161-4e56-95d8-d42fd62753a9"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1785 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The meeting participants are:\n",
            "\n",
            "1. AC Head of Architecture\n",
            "2. BD Managing Director\n",
            "3. AJ Principal Architect\n",
            "4. MM Enterprise Architect\n",
            "5. GC IT Director\n",
            "6. RM Head of Marketing\n",
            "\n",
            "SOURCES:\n",
            "\n",
            "1. synthetic_transcript.vtt\n",
            "2. synthetic_transcript.vtt\n",
            "3. synthetic_transcript.vtt\n",
            "4. synthetic_transcript.vtt\n",
            "5. synthetic_transcript.vtt\n",
            "6. synthetic_transcript.vtt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Actions and Deadlines\n",
        "response = summarize_and_qa.qa(\"describe the meeting follow-on actions and any deadlines\",chain_type=\"map_reduce\")\n",
        "print(response[\"output_text\"])"
      ],
      "metadata": {
        "id": "UAJ0eU1G4XRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fbc06d-0a56-457c-a293-ab15859ce65b"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2178 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The meeting follow-on actions and deadlines are as follows:\n",
            "\n",
            "1. Coordinate with sales team to arrange a workshop for clients (no deadline specified).\n",
            "2. Draft a proposal for the team structure and responsibilities (no deadline specified).\n",
            "3. Evaluate infrastructure needs and scout for potential partnerships in the tech space (report should be ready in about three weeks).\n",
            "4. Collaborate with universities for training and academic insights (no deadline specified).\n",
            "5. Recruit fresh talent through university collaborations (no deadline specified).\n",
            "6. Innovate solutions through fresh perspectives (no deadline specified).\n",
            "7. Set aside a budget for internal R&D (no deadline specified).\n",
            "\n",
            "SOURCES:\n",
            "\n",
            "1. synthetic_transcript.vtt (lines 16-18)\n",
            "2. synthetic_transcript.vtt (lines 26-28)\n",
            "3. synthetic_transcript.vtt (lines 31-33)\n",
            "4. synthetic_transcript.vtt (lines 36-38)\n",
            "5. synthetic_transcript\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Decisions Made\n",
        "response = summarize_and_qa.qa(\"List the decisions made in the meeting\",chain_type=\"map_reduce\")\n",
        "print(response[\"output_text\"])"
      ],
      "metadata": {
        "id": "U0nYjjL94mx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420fad96-8828-49ee-9162-fc6db34bbfeb"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2094 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The decisions made in the meeting are:\n",
            "\n",
            "1. Create a dedicated AI support team.\n",
            "2. Showcase case studies and real-world applications of AI solutions.\n",
            "3. Coordinate with the sales team and arrange a workshop for clients.\n",
            "4. Gather feedback from clients to understand their specific needs in AI solutions.\n",
            "5. Implement a regular training schedule for the team to stay up-to-date with the latest AI advancements.\n",
            "6. Evaluate the company's infrastructure needs for scalable AI.\n",
            "7. Scout for potential partnerships in the tech space.\n",
            "8. Establish a dedicated ethics committee for AI.\n",
            "9. Ensure ethical soundness of products through regular reviews.\n",
            "10. Use \"Ethically Designed AI Solutions\" as a selling point.\n",
            "\n",
            "SOURCES:\n",
            "\n",
            "1. synthetic_transcript.vtt\n",
            "2. synthetic_transcript.vtt\n",
            "3. synthetic_transcript.vtt\n",
            "4. synthetic_transcript.vtt\n",
            "5. synthetic_transcript.vtt\n",
            "6. synthetic_trans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Questions raised\n",
        "response = summarize_and_qa.qa(\"list all the questions raised in the meeting, and answers provided.\",chain_type=\"map_reduce\")\n",
        "print(response[\"output_text\"])"
      ],
      "metadata": {
        "id": "XS0tnZhf8p4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "820de4be-b586-4ef2-aa81-9a6d28e05eed"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2019 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Questions raised in the meeting:\n",
            "\n",
            "1. How to pique interest and drive sales of AI solutions?\n",
            "2. How to understand what clients are specifically looking for in AI solutions?\n",
            "3. How to ensure the team is up-to-date with the latest AI advancements?\n",
            "4. Proposal for the team structure and responsibilities of the dedicated AI support team.\n",
            "5. What are the infrastructure needs for scalable AI?\n",
            "6. Who will lead the team to evaluate infrastructure needs?\n",
            "7. How long will the evaluation take?\n",
            "8. What is the concern regarding AI development?\n",
            "9. What is the suggestion to address security concerns?\n",
            "10. How do we plan to support these AI solutions?\n",
            "11. How can we ensure our products are ethically sound?\n",
            "12. What are the ethical implications of generative AI?\n",
            "\n",
            "Answers provided:\n",
            "\n",
            "1. By demonstrating AI capabilities firsthand through a workshop for clients, showcasing case studies and real-world applications, emphasizing customization and client-specific benefits.\n",
            "2. By gathering feedback from clients.\n"
          ]
        }
      ]
    }
  ]
}