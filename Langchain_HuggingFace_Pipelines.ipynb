{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "private_outputs": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO0v39LaEjJ53JEy4ZcO4ne",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barbaroja2000/llm/blob/main/Langchain_HuggingFace_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Langchain - Hugging Face Pipelines\n",
        "\n",
        "Example Notebook showing how to integrate with models from HuggingFace\n",
        "\n",
        "Requires Hugging face API token:\n",
        "\n",
        "```\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= \"abc\"\n",
        "```"
      ],
      "metadata": {
        "id": "JS-BP6E_T8dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  -q -U langchain  torch \n",
        "!pip install -q -i  https://test.pypi.org/simple/ bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wmSnChLV45bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain import HuggingFacePipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "OHTBYl6PZ405"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "Gl99icgU3Ix_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Keys\n",
        "#@markdown Utitily to load keys from fs, replace with environ vars if not using\n",
        "\n",
        "import os\n",
        "\n",
        "#os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "!python -m pip install python-dotenv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "import dotenv\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/keys/keys.env')\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= os.environ.get(\"HUGGINGFACE_API_KEY\")\n"
      ],
      "metadata": {
        "id": "nPXEpu7HXAAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/models/hf/'\n",
        "\n",
        "!ls -la $TRANSFORMERS_CACHE"
      ],
      "metadata": {
        "id": "yANIKudWv-0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* \"audio-classification\": will return a AudioClassificationPipeline.\n",
        "* \"automatic-speech-recognition\": will return a AutomaticSpeechRecognitionPipeline.\n",
        "* \"conversational\": will return a ConversationalPipeline.\n",
        "* \"feature-extraction\": will return a FeatureExtractionPipeline.\n",
        "* \"fill-mask\": will return a FillMaskPipeline:.\n",
        "\"image-classification\": will return a * ImageClassificationPipeline.\n",
        "* \"question-answering\": will return a QuestionAnsweringPipeline.\n",
        "* \"table-question-answering\": will return a TableQuestionAnsweringPipeline.\n",
        "* \"text2text-generation\": will return a Text2TextGenerationPipeline.\n",
        "* \"text-classification\" (alias \"sentiment-analysis\" available): will return a TextClassificationPipeline.\n",
        "* \"text-generation\": will return a TextGenerationPipeline:.\n",
        "* \"token-classification\" (alias \"ner\" available): will return a TokenClassificationPipeline.\n",
        "* \"translation\": will return a TranslationPipeline.\n",
        "* \"translation_xx_to_yy\": will return a TranslationPipeline.\n",
        "* \"summarization\": will return a SummarizationPipeline.\n",
        "* \"zero-shot-classification\": will return a ZeroShotClassificationPipeline."
      ],
      "metadata": {
        "id": "GxHpqedWrrfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0"
      ],
      "metadata": {
        "id": "tHih4OA7R4Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline.from_model_id(device=device, model_id=\"google/flan-t5-small\", task=\"summarization\", model_kwargs={})"
      ],
      "metadata": {
        "id": "paQdgi__QhvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"{text}\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "text = \"\"\"\n",
        "After James Henry Trotter had been living with his aunts for three\n",
        "whole years there came a morning when something rather peculiar\n",
        "happened to him. And this thing, which as I say was only rather\n",
        "peculiar, soon caused a second thing to happen which was very\n",
        "peculiar. And then the very peculiar thing, in its own turn, caused a\n",
        "really fantastically peculiar thing to occur.\n",
        "It all started on a blazing hot day in the middle of summer. Aunt\n",
        "Sponge, Aunt Spiker and James were all out in the garden. James had\n",
        "been put to work, as usual. This time he was chopping wood for the\n",
        "kitchen stove. Aunt Sponge and Aunt Spiker were sitting comfortably\n",
        "in deck-chairs near by, sipping tall glasses of fizzy lemonade and\n",
        "watching him to see that he didnâ€™t stop work for one moment.\n",
        "Aunt Sponge was enormously fat and very short. She had small\n",
        "piggy eyes, a sunken mouth, and one of those white flabby faces that\n",
        "looked exactly as though it had been boiled. She was like a great\n",
        "white soggy overboiled cabbage. Aunt Spiker, on the other hand, was\n",
        "lean and tall and bony, and she wore steel-rimmed spectacles that\n",
        "fixed on to the end of her nose with a clip. She had a screeching voice\n",
        "and long wet narrow lips, and whenever she got angry or excited,\n",
        "little flecks of spit would come shooting out of her mouth as she\n",
        "talked. And there they sat, these two ghastly hags, sipping their\n",
        "drinks, and every now and again screaming at James to chop faster\n",
        "and faster. They also talked about themselves, each one saying how\n",
        "beautiful she thought she was. Aunt\n",
        "Sponge had a long-handled mirror on her lap, and she kept picking it\n",
        "up and gazing at her own hideous face.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8jniiU-cXGKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "print(llm_chain.run(text))"
      ],
      "metadata": {
        "id": "iSIxbdtaXh_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}