{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMQx9/RKgEZhbqmFfYKS8cy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barbaroja2000/llm/blob/main/OpenLoop_Large_Language_Models_and_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Langchain OpenLoop\n",
        "\n",
        "```\n",
        "import os\n",
        "os.environ.get(\"OPENAI_API_KEY\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "F0tLNCtO6kBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Keys\n",
        "#@markdown Utitily to load keys from fs, replace with environ vars if not using\n",
        "\n",
        "import os\n",
        "\n",
        "#os.environ.get(\"OPENAI_API_KEY\")\n",
        "#os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "!python -m pip install python-dotenv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "import dotenv\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/keys/keys.env')"
      ],
      "metadata": {
        "id": "8xIXPCQ-6H8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain==0.0.208 deeplake openai tiktoken > /dev/null"
      ],
      "metadata": {
        "id": "wmSnChLV45bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to LLMs"
      ],
      "metadata": {
        "id": "mYTRHFOAkJuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "text = \"What would be a good company name for a company that recommends Sports Bets?\"\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    result = llm(text)\n",
        "    print(result, cb)"
      ],
      "metadata": {
        "id": "GOm2g30q_1Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the weather like?\",\n",
        "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
        "    }, {\n",
        "        \"query\": \"How old are you?\",\n",
        "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create an example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is known for its humor and wit, providing\n",
        "entertaining and amusing responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few-shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "KXp8cNRJglUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "\n",
        "# load the model\n",
        "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
        "chain.run(\"What's the meaning of life?\")"
      ],
      "metadata": {
        "id": "WoABuR0ugput"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "id": "D7qqCWjyg9CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA"
      ],
      "metadata": {
        "id": "4Jo-OkfyjjGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "        template=template,\n",
        "    input_variables=['question']\n",
        ")\n",
        "\n",
        "# user question\n",
        "question = \"What is the capital city of France?\""
      ],
      "metadata": {
        "id": "cbHGhDyAhAo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize Hub LLM\n",
        "hub_llm = HuggingFaceHub(\n",
        "        repo_id='google/flan-t5-large',\n",
        "    model_kwargs={'temperature':0}\n",
        ")\n",
        "\n",
        "# create prompt template > LLM chain\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=hub_llm\n",
        ")\n",
        "\n",
        "# ask the user question about the capital of France\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "ZqgvrWshhDnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = [\n",
        "    {'question': \"What is the capital city of France?\"},\n",
        "    {'question': \"What is the largest mammal on Earth?\"},\n",
        "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
        "    {'question': \"What color is a ripe banana?\"}\n",
        "]\n",
        "res = llm_chain.generate(qa)\n",
        "\n",
        "for generation in res.generations:\n",
        "  print(generation[0].text)\n",
        "\n"
      ],
      "metadata": {
        "id": "kxTUTOKQhLm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"What is the capital city of France?\\n\" +\n",
        "    \"What is the largest mammal on Earth?\\n\" +\n",
        "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
        "\t\t\"What color is a ripe banana?\\n\"\n",
        ")\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "GjTuUhv3iyVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization"
      ],
      "metadata": {
        "id": "zjZMw2HbjfiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "no7vhJ77jQPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
        "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
        "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
      ],
      "metadata": {
        "id": "OfkZqu1fjUnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
        "summarized_text = summarization_chain.predict(text=text)"
      ],
      "metadata": {
        "id": "11gMzLU-jW_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarized_text)"
      ],
      "metadata": {
        "id": "GF9wflXGjanc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Translation"
      ],
      "metadata": {
        "id": "ta8yh3Fnjluo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
        "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
        "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
      ],
      "metadata": {
        "id": "kc3PpCY-jpaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_language = \"English\"\n",
        "target_language = \"French\"\n",
        "text = \"Your text here\"\n",
        "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
      ],
      "metadata": {
        "id": "_aJNE7dzjsM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4LwKL3HjqNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Tokens"
      ],
      "metadata": {
        "id": "PJYOeIrHkOk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers > /dev/null"
      ],
      "metadata": {
        "id": "tHRN-bSmkSmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Download and load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "okkZBc0Qkd6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( tokenizer.vocab )"
      ],
      "metadata": {
        "id": "Ymtu9YkCkiVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.encode(\"This is a sample text to test the tokenizer.\")\n",
        "\n",
        "print( \"Tokens:   \", tokenizer.convert_ids_to_tokens( token_ids ) )\n",
        "print( \"Token IDs:\", token_ids )"
      ],
      "metadata": {
        "id": "14TnWJHCkrbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buliding Applications powered by LLMs"
      ],
      "metadata": {
        "id": "1xrV2w_Gk5Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "template = \"You are an assistant that helps users find information about movies.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"Find information about the movie {movie_title}.\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "response = chat(chat_prompt.format_prompt(movie_title=\"Inception\").to_messages())\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "6AGxK11ik89b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf > /dev/null"
      ],
      "metadata": {
        "id": "guttdykOmGUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize language model\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "# Load the summarization chain\n",
        "summarize_chain = load_summarize_chain(llm)\n",
        "\n",
        "# Load the document using PyPDFLoader\n",
        "loader = PyPDFLoader(file_path=\"drive/MyDrive/test/the-twits-by-roald-dahl.pdf\")\n",
        "\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "# Summarize the document\n",
        "summary = summarize_chain([pages[0]])\n",
        "print(summary['output_text'])"
      ],
      "metadata": {
        "id": "lh8zrSHelIZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the World of Language Models: LLMs vs Chat Models"
      ],
      "metadata": {
        "id": "o9qmpHQEpNcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.3)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "  input_variables=[\"product\"],\n",
        "  template=\"What is a good name for a company that  recommends  {product}?\",\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print( chain.run(\"sports bets\") )"
      ],
      "metadata": {
        "id": "9sdPZOG6pZw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "  HumanMessage,\n",
        "  SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "\tSystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "\tHumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
        "]\n",
        "\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "oskp_lq_pnfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_messages = [\n",
        "  [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "    HumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
        "  ],\n",
        "  [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates French to English.\"),\n",
        "    HumanMessage(content=\"Translate the following sentence: J'aime la programmation.\")\n",
        "  ],\n",
        "]\n",
        "print( chat.generate(batch_messages) )"
      ],
      "metadata": {
        "id": "URB3wdmapwc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Conversational Capabilities with GPT-4 and ChatGPT"
      ],
      "metadata": {
        "id": "08S9ohX4p8RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"What is the capital of France?\"),\n",
        "    AIMessage(content=\"The capital of France is Paris.\")\n",
        "]"
      ],
      "metadata": {
        "id": "XW1ZLyYQqBPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=\"I'd like to know more about the city you just mentioned.\"\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "response = llm(messages)"
      ],
      "metadata": {
        "id": "rEWvwox_qFrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "g1cqnnT5qOoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(response)"
      ],
      "metadata": {
        "id": "xTNMC-bQqotY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (messages)"
      ],
      "metadata": {
        "id": "FRXMczjLqcgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a News Articles Summarizer"
      ],
      "metadata": {
        "id": "QMf29GpNsjsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q newspaper3k python-dotenv"
      ],
      "metadata": {
        "id": "xYnFfNG8sloa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from newspaper import Article\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
        "}\n",
        "\n",
        "article_url = \"https://www.artificialintelligence-news.com/2022/01/25/meta-claims-new-ai-supercomputer-will-set-records/\"\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "try:\n",
        "    response = session.get(article_url, headers=headers, timeout=10)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        article = Article(article_url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        print(f\"Title: {article.title}\")\n",
        "        print(f\"Text: {article.text}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch article at {article_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred while fetching article at {article_url}: {e}\")"
      ],
      "metadata": {
        "id": "PILz5ITcstL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    HumanMessage\n",
        ")\n",
        "\n",
        "# we get the article data from the scraping part\n",
        "article_title = article.title\n",
        "article_text = article.text\n",
        "\n",
        "# prepare template for prompt\n",
        "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
        "\n",
        "Here's the article you want to summarize.\n",
        "\n",
        "==================\n",
        "Title: {article_title}\n",
        "\n",
        "{article_text}\n",
        "==================\n",
        "\n",
        "Write a summary of the previous article.\n",
        "\"\"\"\n",
        "\n",
        "prompt = template.format(article_title=article.title, article_text=article.text)\n",
        "\n",
        "messages = [HumanMessage(content=prompt)]"
      ],
      "metadata": {
        "id": "rle_WOUPs0Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# load the model\n",
        "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0)"
      ],
      "metadata": {
        "id": "GA4IIWIWs3V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate summary\n",
        "summary = chat(messages)\n",
        "print(summary.content)"
      ],
      "metadata": {
        "id": "RhC9q3xMs6z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare template for prompt\n",
        "template = \"\"\"You are an advanced AI assistant that summarizes online articles into bulleted lists.\n",
        "\n",
        "Here's the article you need to summarize.\n",
        "\n",
        "==================\n",
        "Title: {article_title}\n",
        "\n",
        "{article_text}\n",
        "==================\n",
        "\n",
        "Now, provide a summarized version of the article in a bulleted list format.\n",
        "\"\"\"\n",
        "\n",
        "# format prompt\n",
        "prompt = template.format(article_title=article.title, article_text=article.text)\n",
        "\n",
        "# generate summary\n",
        "summary = chat([HumanMessage(content=prompt)])\n",
        "print(summary.content)"
      ],
      "metadata": {
        "id": "JgQDmfN3tC1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare template for prompt\n",
        "template = \"\"\"You are an advanced AI assistant that summarizes online articles into bulleted lists in French.\n",
        "\n",
        "Here's the article you need to summarize.\n",
        "\n",
        "==================\n",
        "Title: {article_title}\n",
        "\n",
        "{article_text}\n",
        "==================\n",
        "\n",
        "Now, provide a summarized version of the article in a bulleted list format, in French.\n",
        "\"\"\"\n",
        "\n",
        "# format prompt\n",
        "prompt = template.format(article_title=article.title, article_text=article.text)\n",
        "\n",
        "# generate summary\n",
        "summary = chat([HumanMessage(content=prompt)])\n",
        "print(summary.content)"
      ],
      "metadata": {
        "id": "vqhEv0lZtISV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the Open-Source GPT4All Model Locally"
      ],
      "metadata": {
        "id": "BDh2wMuTtQXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "local_path = 'drive/MyDrive/models/gpt4all-lora-quantized-ggml.bin'\n",
        "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
        "\n",
        "# send a GET request to the URL to download the file.\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# open the file in binary mode and write the contents of the response\n",
        "# to it in chunks.\n",
        "with open(local_path, 'wb') as f:\n",
        "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "        if chunk:\n",
        "            f.write(chunk)"
      ],
      "metadata": {
        "id": "vPSMH6PjtTJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "!cd llama.cpp && git checkout 2b26469\n",
        "!python3 drive/MyDrive/models/llama.cpp/convert.py drive/MyDrive/models/gpt4all-lora-quantized-ggml.bin"
      ],
      "metadata": {
        "id": "atJKF8eGt5Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "cbfIfeJ9u6Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 llama.cpp/convert.py drive/MyDrive/models/gpt4all-lora-quantized-ggml.bin"
      ],
      "metadata": {
        "id": "okQwOXwbu0Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q langchain==0.0.152"
      ],
      "metadata": {
        "id": "HRepjj83wtHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q pyllamacpp==1.0.7"
      ],
      "metadata": {
        "id": "f9WtzkgZwwdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "id": "hOoqFdXtwxqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "NJMvR2jxxFt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.callbacks.base import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "kiV7sF3nw2WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "YKtIhZH-xk3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "llm = GPT4All(model=\"drive/MyDrive/models/ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True)\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "zx9CIFw9xsIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What happens when it rains somewhere?\"\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "id": "pQlhILfQzbBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "KgsP3_J3zfFN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}